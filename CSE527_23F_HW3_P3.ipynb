{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSE527 Homework 3 - Part 3\n",
        "**Due date: 23:59 on Nov 26, 2023**\n",
        "\n",
        "\n",
        "## Description\n",
        "---\n",
        "Segmentation has been recently gaining more popularity in the field of computer vision. In this homework you will be implementing a standard UNet model to perform semantic segmentation. While transofrmer based architectures have demostrated impressive perfromance in semantic/instance segmentation, it's important to be familiar with other architectures commonly used for semantic segmentation.\n",
        "\n",
        "You will use the same dataset that you have used in part 2 of this homework, but the output of the model here is a class level segmentation mask.\n",
        "\n",
        "Input of the model will be an RGB image (3xHxW) and output will be a boolean mask (HxW).\n",
        "\n",
        "| Input Image      | Output Mask (class:horse) |\n",
        "| ----------- | ----------- |\n",
        "| ![Input Image](https://drive.google.com/uc?id=1BqmNxzV4t5MFQHjBFizlI1p9xlEatXiz)| ![Output Image](https://drive.google.com/uc?id=1NCW95D8JDBNsgzfFNa14ZiV_SiCpBPIv)|\n",
        "\n",
        "  \n",
        "Read the paper: [U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "](https://arxiv.org/abs/1505.04597) to understand the structure of UNet models, what they are composed of, and how to build them.  \n",
        "\n",
        "Also you should refer to this github repo to understand the implementations of a UNet: [milesial/Pytorch-UNet](https://github.com/milesial/Pytorch-UNet) . Specifically refer to unet/unet_model.py to gain understanding of the convolutional blocks, and skip connections.\n",
        "\n",
        "In this part you will be designing UNet architecture simmilar to it but with specific changes."
      ],
      "metadata": {
        "id": "evO0uWTFuVEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code\n",
        "---\n"
      ],
      "metadata": {
        "id": "_AUdZulcEhKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HW3_ROOT_PATH = '/gdrive/My Drive/' + 'CSE527/hw3/' #'YOUR HOMEWORK3 PATH HERE'\n",
        "PATH_TO_PART3 = HW3_ROOT_PATH + 'part3/'\n",
        "PATH_TO_COCO = HW3_ROOT_PATH + 'coco/'"
      ],
      "metadata": {
        "id": "CaC60Q7aG827"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You will be using this code [unet.zip](https://drive.google.com/file/d/1p2tMMUfF2vfcr3QHipJx0XyiBOMWoMNM/view?usp=sharing) to complete your work. This code contains all the tools for loading dataset, training the newtorks, loss methods and evaluation metrics.\n",
        "\n",
        "\n",
        "To pull this code into your drive, you have to first add the zip file as shortcut to current working directory (PATH_TO_PART3 above). To do this, open the above link and click on \"Add shortcut to Drive\" button (drive symbol with a plus) and navigate to working PATH_TO_PART3 directory and add shortcut.\n",
        "\n",
        "\n",
        "We will use the same dataset that you have used in the Part 2 : [coco.zip dataset](https://drive.google.com/file/d/1GVyxYHwVgiG9z_Sn46wslT_2n65DLZRw/view?usp=sharing). Put this in the root of your hw3 HW3_ROOT_PATH. If you have this extracted to some path you can re-use the it.\n"
      ],
      "metadata": {
        "id": "65kgaM0gJqv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "# CD into root of your homework2 part3 directory\n",
        "%cd -q $HW3_ROOT_PATH\n",
        "!mkdir -p '{PATH_TO_PART3}'\n",
        "%cd $PATH_TO_PART3"
      ],
      "metadata": {
        "id": "t9q54KFTDo5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a707b982-459d-4f68-e1fb-e0646349e431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/CSE527/hw3/part3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vgMRohwGdhz",
        "outputId": "d84d797d-012a-46ec-c627-2a2dc4bb92b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unet  unet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "metadata": {
        "id": "BznyfGgxaiQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n ../coco.zip -d ..\n",
        "!unzip -n unet.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a3TSbTVGfQq",
        "outputId": "bb607e1d-5ffb-4bb8-c906-81b5459cf2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ../coco.zip\n",
            "Archive:  unet.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd -q unet\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Q1JskhL7xx",
        "outputId": "4169df85-0866-46e4-921f-5fbde26a21d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco.py  main.py  __pycache__  transforms.py  unet.py  util\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load helper method"
      ],
      "metadata": {
        "id": "e7UKBNHZkY0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def show_images(folder_path):\n",
        "    image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    image_paths = image_paths[:50]\n",
        "    rows = 5\n",
        "    cols = 10\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        image = plt.imread(image_path)\n",
        "        row = int(i / cols)\n",
        "        col = i % cols\n",
        "        axes[row, col].imshow(image)\n",
        "        axes[row, col].set_title(os.path.basename(image_path))\n",
        "        axes[row, col].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "WTMldPBmkYSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "---\n",
        "You will use main.py for training, evaluating and visualizing.\n",
        "\n"
      ],
      "metadata": {
        "id": "kmF7HRekODRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a31mgY5yMAkV",
        "outputId": "c6f0993a-dfd2-458a-a351-794cf8dce25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device cuda\n",
            "usage: main.py [-h] [--coco_path COCO_PATH] [--batch_size BATCH_SIZE] [--epochs EPOCHS] [--eval]\n",
            "               [--load_pretrained] [--skip {concat,sum}] [--checkpoint CHECKPOINT]\n",
            "\n",
            "Train a segmentation model on COCO data.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --coco_path COCO_PATH\n",
            "                        Path to COCO dataset directory.\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training.\n",
            "  --epochs EPOCHS       Epochs for training.\n",
            "  --eval                Whether to evaluate the model on the validation set.\n",
            "  --load_pretrained     Whether to evaluate the model on the validation set.\n",
            "  --skip {concat,sum}   Skip connection strategy: either concatenation (concat) or summation\n",
            "                        (sum).\n",
            "  --checkpoint CHECKPOINT\n",
            "                        Path to checkpoint file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement UNet (15 points)\n",
        "---\n",
        "\n",
        "\n",
        "`unet.py` contains two classes `UNet()` and `SumUNet()`, both are implementations of `torch.nn.Module()`.\n",
        "\n",
        "You need to complete the methods `UNet:__init__()` and `UNet:forward()` to implement a segmentation architecture that processes input of shape `N x 3 x H x W` (N is batch size) and will output a mask of shape `N x 1 x H x W`\n",
        "\n",
        "###Requirements\n",
        "1. Encoder part of the UNet will be made of pytorch Resnet's layers 1 to 4.\n",
        "\n",
        "    https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html\n",
        "\n",
        "    Essentially, your UNet's encoder will be using `torchvision.models.resnet`'s `layer1, layer2, layer3, layer4` for downsampling.\n",
        "\n",
        "2. When argument `load_pretrained_encoder_layers` of __init__() is set to true, the layers will be initialized with the pretrained weights. You can use the default pre-trained weights provided by pytorch.\n",
        "\n",
        "3. As described in the paper or above repo, `concatenation` operations will be done for the skip connections. (in the upcoming section, you will instead use summation)\n",
        "\n",
        "\n",
        "Note:\n",
        "- You are recommended to size your model under 25M paramenters. As more parameters will hurt the trainig time and even effeciency.\n",
        "- For the decoder section (later half), you will not be able to use any pre-trained weights, so you will use randomly initialized blocks (that you will design on your own). Each block usually have layers of `[Conv, BatchNorm, ReLu]`\n",
        "- You need not use any ReLU on the last mask output layer.\n",
        "\n",
        "Hint:\n",
        "- chose a [resnet](https://pytorch.org/vision/main/_modules/torchvision/models/resnet.html) size\n",
        "- understand the `In` channels & `Out` channels of each layer of your chosen resnet()\n",
        "- build decoder layers so that they will work along the above 4 layers\n",
        "- conclude how to use the skip connections. ( you will using `concatenation`).\n",
        "- complete the forward() method\n",
        "- use print(x.shape) statements to understand the flow of tensors\n",
        "\n",
        "Log reference: [log](https://drive.google.com/file/d/1ZGd-ciOKkajri0GEOwc-uJDBjGJHJdNY/view?usp=drive_link)\n",
        "\n",
        "Aim for mIoU more than 0.35. You may not be penalized for low perfromance of your model.  \n",
        "\n"
      ],
      "metadata": {
        "id": "HGAEVeaQPsLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip concat --coco_path \"{PATH_TO_COCO}\""
      ],
      "metadata": {
        "id": "TIprq1FLPhiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "To just run the evaluation or to produce the predicted masks in the `images/` folder, you need to run the main.py with --eval.\n",
        "Note that this will load the latest stored checkpoint corresponding to the --skip flag that you have provided."
      ],
      "metadata": {
        "id": "Wzd-8LzheCsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip concat --coco_path \"{PATH_TO_COCO}\" --eval"
      ],
      "metadata": {
        "id": "q7-dygRzdPjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With --load_pretrained\n",
        "\n",
        "When properly configured using pre-trained weights should result in higher mIoUs. You can expect mIoU around .65  "
      ],
      "metadata": {
        "id": "CplyPkw6rEIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip concat --coco_path \"{PATH_TO_COCO}\"  --load_pretrained"
      ],
      "metadata": {
        "id": "yINvWiCmdcsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip concat --coco_path \"{PATH_TO_COCO}\" --eval"
      ],
      "metadata": {
        "id": "z2tNLNZdeclc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the masks generated by the best run:\n",
        "It is okay if the masks are not perfectly aligned. We are only training over 500 images for a short duration!"
      ],
      "metadata": {
        "id": "nFfKAThQfMmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'images'\n",
        "show_images(folder_path)"
      ],
      "metadata": {
        "id": "A2tk7lW6fSGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement SumUNet (10 points)\n",
        "---\n",
        "This is very much similar to above UNet, but for the skip connections we will be using summation operation instead of concatenation. This is kind of an open ended question, in the sense that there are multuple ways you can design connections.\n",
        "\n",
        "Feel free to use any layers | blocks | up/down sampling.\n",
        "\n",
        "\n",
        "Hint:\n",
        "- Again you need to focus on the feature shapes\n",
        "- you can use sampling techniques to change the height/width\n",
        "- you can use conv layers to change number of channels  \n",
        "\n",
        "Note: You can chose to remove --load_pretrained from the below cell if you think that will have a better perfromance\n"
      ],
      "metadata": {
        "id": "X6L1Sbfngc4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip sum --coco_path \"{PATH_TO_COCO}\"  --load_pretrained"
      ],
      "metadata": {
        "id": "uHP8E-5siqHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --skip sum --coco_path \"{PATH_TO_COCO}\"  --eval"
      ],
      "metadata": {
        "id": "6QLZ-O2mkNvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission guidelines\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The submission structure should look like:\n",
        "\n",
        "IMPORTANT: Remove the images/ folder, and any CHECKPOINT files.\n",
        "\n",
        "\n",
        "```\n",
        "{last name}_{first name}_{sbu id}_hw2/\n",
        "├── part1/\n",
        "│   └── ...\n",
        "├── part2/\n",
        "│   └── ...\n",
        "├── part3/\n",
        "│   ├── CSE527_23F_HW3_P3.ipynb\n",
        "│   └── unet/\n",
        "│       ├── utils/\n",
        "│       │   ├── ...\n",
        "│       │   └── ...\n",
        "│       ├── coco.py\n",
        "│       ├── main.py\n",
        "│       ├── transforms.py\n",
        "│       └── unet.py\n",
        "```\n",
        "\n",
        "\n",
        "Follow instructions in part1's submission guidelines to generate your complete submission"
      ],
      "metadata": {
        "id": "wciR_LRjk9k9"
      }
    }
  ]
}