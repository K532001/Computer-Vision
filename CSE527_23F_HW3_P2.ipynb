{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A76y1-yONKP-"
      },
      "source": [
        "# CSE527 Homework 3 - Part 2\n",
        "**Due date: 23:59 on Nov 26, 2023**\n",
        "\n",
        "\n",
        "## Description\n",
        "---\n",
        "Detection is a fundamental task in CV. This homework is on based on DETR (https://arxiv.org/abs/2005.12872), one of the most famous yet simple detection model.\n",
        "\n",
        "In this part, you will get familiar with DETR code base and will be modfiying the code (https://github.com/facebookresearch/detr/tree/main) to perfrom a new task: **_Detection using Centroids_**.\n",
        "\n",
        "Traditionally bounding boxes are used to highlight detections, however in this homework you will perfrom detection of an object by selecting a _meaningful_ center point on it. We will call that point the centroid. For applications involving peculiar objects (like Giraffes,), where the center of the bounding box may not lie on the objects main body, a centroid detection may be more useful.    \n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgGY71C1RotX"
      },
      "source": [
        "## Code\n",
        "---\n",
        "Although DETR is simple relative to other detection models, the code to train and test is still complex and is spread acorss multiple directories. For this reason, the code is provided in a zipped package (here [detr.zip](https://drive.google.com/file/d/1GuserI8EDxAlTG4eh98JhugVsSDWCS3f/view?usp=sharing)).\n",
        "\n",
        "To pull this code into your colab, you have to first add the zip file as shortcut to current working directory. To do this, open the above link and click on \"Add shortcut to Drive\" button (drive symbol with a plus) and navigate to working current directory and add shortcut.\n",
        "\n",
        "You have to repeat the same process to download this [coco.zip dataset](https://drive.google.com/file/d/1GVyxYHwVgiG9z_Sn46wslT_2n65DLZRw/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HW3_ROOT_PATH = '/gdrive/MyDrive/' + 'CSE527/hw3/' #'FILL IN YOUR HOMEWORK3 ROOT HERE'\n",
        "PATH_TO_PART2 = HW3_ROOT_PATH + 'part2/'\n",
        "PATH_TO_COCO = HW3_ROOT_PATH + 'coco/'\n",
        "# TRY to use these variable wherever you load/save some file"
      ],
      "metadata": {
        "id": "w1Q9PhJ2VBsQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33_hqc4mAHgr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "# CD into root of your homework2 part3 directory\n",
        "%cd -q $PATH_TO_PART2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78HdhksulDFl"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oggi2izQAbSX"
      },
      "outputs": [],
      "source": [
        "!unzip -n detr.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LEZ1QmRAd2g"
      },
      "outputs": [],
      "source": [
        "%cd detr\n",
        "%mkdir checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "---\n",
        "If you are using colab, running below cell will suffice, else refer to requirements.txt to install all dependencies"
      ],
      "metadata": {
        "id": "XpJQT7nBtFE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtvGJxdFBAjy"
      },
      "outputs": [],
      "source": [
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model\n",
        "---\n",
        "DETR consists of\n",
        "- a feature extractor,\n",
        "- transformer and\n",
        "- two output heads\n",
        "    - classification head\n",
        "    - regression head\n",
        "\n",
        "In the original implementation, regression head is designed to output the bouding box attributes ltrb (left, top, right, & bottom edges of object). In this homework we intend to train the model to output the centeroids of each object.\n",
        "\n",
        "###How do we define centroids:###\n",
        "Original DETR uses the MS COCO's _bbox_ annotations to train the model, while in this HW we will use the _segmentation_ annotations.\n",
        "Each image may have multiple objects. The pixel outline of each object is provided as an annotation. We will use this contour information to generate the object mask.\n",
        "\n",
        "\n",
        "In the below image, you can see that centroid is near the biggest portion of the object body.  \n",
        "![](https://drive.google.com/uc?export=view&id=1CNym_GFV8EKwTQPZQSHZdPB7P-3qarEm)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f78Q5j1uxxmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get yourself familiar with Data set\n",
        "---\n",
        "\n",
        "To get your self familiar with the dataset, you are encouraged to play around with the COCO API. The sample code  given below displays the segmentation data format. In the later sections you will be yusing the segmentation data to estimate a centroid.  "
      ],
      "metadata": {
        "id": "ME_ZLEF2MStQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "from hubconf import _make_detr\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "coco = COCO('instances_train2017.json')\n",
        "ids = coco.getImgIds()\n",
        "sample = 0\n",
        "im = coco.loadImgs(ids[sample])[0]\n",
        "print(f'Image keys: {im.keys()}')\n",
        "annos = coco.loadAnns(coco.getAnnIds(ids[sample]))\n",
        "# Note: segmentation annotation is in the format of [x1, y1, x2, y2, x3, y3, . . . ], where (x1, y1) is a point\n",
        "# on countour, x being dist from left and y is dist from top row of image\n",
        "print(f'Anno segmentation: {(annos[0][\"segmentation\"])}')\n",
        "############# (0 points) ########################\n",
        "\n",
        "\n",
        "#################################################\n"
      ],
      "metadata": {
        "id": "MQ4kPTvLMwTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete the code\n",
        "---\n",
        "\n",
        "This provided detr/ folder has the following structure\n",
        "```\n",
        "detr/\n",
        "├── datasets/\n",
        "│   ├── coco.py\n",
        "│   ├── ...\n",
        "├── models/\n",
        "│   ├── detr.py\n",
        "│   ├── matcher.py\n",
        "│   └── ...\n",
        "├── utils/\n",
        "│   └── ...\n",
        "├── engine.py\n",
        "├── main.py\n",
        "├── requirements.txt\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "In these python files, you will be implementing the missing code blocks. These blocks are highlited by the TODOs\n",
        "```#### TODO CSE527 STUDENT CODE STARTS HERE ######```\n",
        "\n",
        "\n",
        "In total there are 5 sections the code has to be filled. They are located in the files: coco.py, detr.py, matcher.py.  \n",
        "\n",
        "To complete this part you are very much encouraged to:\n",
        "- understand how dataset, data sampler, dataloader, model, loss, & optimizer work together during training  \n",
        "- refer to the actual DETR implementation at [git:facebookresearch/detr](https://github.com/facebookresearch/detr/tree/main) to get an idea of the training pipeline.\n",
        "- try to understand flow of control from main.py.\n",
        "\n",
        "You cannot complete the missing code blocks just from the given description in this ipynb, so you must read the code around. That is, you will read the inputs, outputs and how the inputs to the code block are created and how the outputs from the code block will be utilized.   \n",
        "\n",
        "## Task\n",
        "You should modify the DETR to output centroids rather than bounding boxes.   \n",
        "Dataset:\n",
        "At the dataset level, you are not actually given the centroids, so you will use the instance segmentation masks to estimage the centroid for each object in annotation.\n",
        "\n",
        "Model:\n",
        "For this you will need to replace the classification and regression heads (already removed in the code provided to you, you just need to add new).\n",
        "\n",
        "\n",
        "\n",
        "### 1 - coco.py: ConvertCocoPolysToMask.\\__call__()  [20 points]\n",
        "Here you will be writing the logic to convert annotations to centroids in the following method.\n",
        "Note: You will have to figure out the inputs and outputs by yourself.  \n",
        "```\n",
        "def convert_coco_poly_to_centroids(segmentations, h, w, boxes) -> Torch.Tensor(shape = num_segmentations x 2)\n",
        "```\n",
        "\n",
        "This method is part of data loader, and is called when generating the <sample, prediction> pairs, which will eventually be passed onto model for training.\n",
        "\n",
        "The _centroid_ we are referring to is a hypothetical point on the center of the object's main body part. It should ideally never be a point outside the annotation mask, and is likley always in the middle of body. Here in our case centroid should lie on the middle of horse's torso.\n",
        "\n",
        "steps:\n",
        " - convert segmentations into masks [5 points]\n",
        " - figure out a way to remove thin sections of object (legs and tail of the horse) from the mask, so only the largest body part (torso) will influence the centroid [10 points]\n",
        " - compute the center of mass of the mask [5 point]\n",
        "\n",
        " Feel free to pass on more inputs/outputs to the above method to support your way of computing centroids.\n",
        "\n",
        "\n",
        "### 2 & 3 - detr.py: DETR.\\__init__(), DETR.forward() [10 points]\n",
        "\n",
        "This is the DETR model architectrure. Here you will change the classification and regression heads, so the model can output centroids (unlike bboxes in original implementation).\n",
        "Also you will write logic to freeze the remaining section of DETR.\n",
        "\n",
        "Original model takes 300 hours to completely train from scrtch, so it is important to freeze all weights except necessary ones (heads).\n",
        "\n",
        "### 4 - matcher.py: HungarianMatcher.forward() [15 points]\n",
        "\n",
        "Matcher is a key part of detection models. Refer to the paper or original DETR implementation to see how/why matcher is used. In this implementation we use Hungarian algorithm to align predictions to the ground truths.\n",
        "\n",
        "Breif:  \n",
        "All object instance predictions are matched against the ground truth objects. This is necessary to compute the loss.\n",
        "Basically matcher will guess which ground truth object each prediction might be referring based on the closeness of predicted centroids and ground truth centroids.\n",
        "\n",
        "100 centroid predictions will be matched against the ground truth centroids. You will compute a cost matrix that encdes the relation of prediction to GT.\n",
        "\n",
        "Refer: https://en.wikipedia.org/wiki/Hungarian_algorithm\n",
        "\n",
        "How you compute the cost matrix is again upto you.\n",
        "\n",
        "### 5 - detr.py: SetCriterion.loss_centroid() [10 points]\n",
        "\n",
        "Here you will implement the loss function. Given the predicted centroids and ground truth centroids (which you processed in coco.py), you will write logic to compute variable _centroid_loss_\n",
        "\n",
        "The loss method is up to you.  \n",
        "\n",
        "Note: At code blocks 4 & 5, make sure the loss/cost is in the normalized range only. I.e., loss/cost is calculated on normalized inputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dY5xgrbNtUPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing centroid\n",
        "---\n",
        "\n",
        "Below block provide you a sandbox environment to play around and create a centroid finding algorithm. You will eventually use this code at detr/dataset/coco.py to complete the traning pipeline.\n",
        "\n",
        "Try to produce centroids that are within 10 pixels a far from the provided gt centroids."
      ],
      "metadata": {
        "id": "PEUnIZYiPulV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convert_coco_poly_to_centroids(segmentations, height, width, bboxes):\n",
        "    \"\"\"Converts COCO polygon annotations to centroids.\n",
        "    Args:\n",
        "        segmentations: A list of COCO polygon annotations.\n",
        "        height: The height of the image.\n",
        "        width: The width of the image.\n",
        "        boxes: bounding boxes\n",
        "\n",
        "    Returns:\n",
        "        A PyTorch tensor containing the centroids of the objects in the image.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "qsMQKfh_nogq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_centroids(img, anns):\n",
        "    h, w = img['height'], img['width']\n",
        "    # Get segmentations and convert them to centroids\n",
        "    segmentations = [ann['segmentation'] for ann in anns]\n",
        "    boxes = [obj[\"bbox\"] for obj in anns]\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "    boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "    return convert_coco_poly_to_centroids(segmentations, h, w, boxes)\n",
        "\n",
        "\n",
        "def compute_drift_rate(centroids, gt_centroids, gt_areas):\n",
        "  distance_matrix = torch.cdist(centroids, gt_centroids)\n",
        "  distances = []\n",
        "  equivalent_lengths = []\n",
        "  for i in range(len(centroids)):\n",
        "      min_distance = torch.min(distance_matrix[i])\n",
        "      min_distance_index = torch.argmin(distance_matrix[i])\n",
        "      distances.append(min_distance)\n",
        "      equivalent_lengths.append(np.sqrt(gt_areas[min_distance_index]))\n",
        "  return torch.div(torch.Tensor(distances), torch.Tensor(equivalent_lengths))\n",
        "\n",
        "\n",
        "# driver code\n",
        "img_id = 144505\n",
        "img = coco.loadImgs(img_id)[0]\n",
        "img_data = Image.open(requests.get(img['coco_url'], stream=True).raw)\n",
        "\n",
        "# Get image annotations\n",
        "anns = coco.loadAnns(coco.getAnnIds(img_id))\n",
        "centroids = estimate_centroids(img, anns)\n",
        "gt_centroids = torch.Tensor([[378, 298],[537, 258]])\n",
        "print(f'Predicted centroids: {centroids}')\n",
        "plt.imshow(img_data)\n",
        "for centroid in centroids:\n",
        "    plt.plot(centroid[0], centroid[1], 'ro')\n",
        "for centroid in gt_centroids:\n",
        "    plt.plot(centroid[0], centroid[1], 'bo')\n",
        "# Show image\n",
        "plt.title(f'Drift dist = {compute_drift_rate(centroids, gt_centroids, [ann[\"area\"] for ann in anns])}')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ME45YOYjQOXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "---\n",
        "\n",
        "Implement all five mentioned code blocks and use the below command AS IS. You can change BS, num_epochs, num_workers. (use this output training  [log](https://drive.google.com/file/d/1ZGd-ciOKkajri0GEOwc-uJDBjGJHJdNY/view?usp=drive_link) for your reference).\n",
        "\n",
        "Make sure checkpoints are being saved correctly and sometime not mistakenly being overriden due to multiple executions.\n",
        "\n",
        "You can use the validation loss/loss_centroid  as a metric to track performance.\n",
        "\n",
        "For grading:\n",
        "- There are no strict limits on evaluation metrics.\n",
        "- In the last cell of this notebook _avg_drift_rate_ metric is computed on a subset of validation ds. Try to score below 0.35.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h_zLwD3_YUPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSSw5hYkBhk_"
      },
      "outputs": [],
      "source": [
        "!python -u main.py --coco_path $PATH_TO_COCO --batch_size 10 --num_workers 12 --epochs 20 --output_dir './checkpoints'  --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --freeze_body"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing\n",
        "---------------"
      ],
      "metadata": {
        "id": "4TgkgtGQZJbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8I-R5LTnFfGS"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def rescale_center(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = out_bbox * torch.tensor([img_w, img_h], dtype=torch.float32, device=device)\n",
        "    return b\n",
        "\n",
        "def detect(im, model, transform, det_threshold = 0.75):\n",
        "    # mean-std normalize the input image (batch-size: 1)\n",
        "    img = transform(im).unsqueeze(0)\n",
        "\n",
        "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
        "    # if you want to use images with an aspect ratio outside this range\n",
        "    # rescale your image so that the maximum size is at most 1333 for best results\n",
        "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
        "\n",
        "    # propagate through the model\n",
        "    outputs = model(img.to(device))\n",
        "\n",
        "    # keep only predictions with 0.7+ confidence\n",
        "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > det_threshold\n",
        "\n",
        "    # convert boxes from [0; 1] to image scales\n",
        "    centroids = rescale_center(outputs['pred_centroids'][0, keep], im.size)\n",
        "    return probas[keep].detach().cpu(), centroids.detach().cpu()\n",
        "\n",
        "def getImg(url):\n",
        "    return Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "def is_grayscale(image):\n",
        "    if image.mode == \"L\":\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def convert_grayscale_to_rgb(image):\n",
        "    if not is_grayscale(image):\n",
        "        return image\n",
        "    else:\n",
        "        return image.convert(\"RGB\")\n",
        "\n",
        "\n",
        "def plot_results(pil_img, prob, centroids, gt_centroids, boxes, file_name):\n",
        "    # plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    ax.scatter([c[0] for c in gt_centroids], [c[1] for c in gt_centroids], color='blue', s=10)\n",
        "    ax.scatter([c[0] for c in centroids], [c[1] for c in centroids.cpu()], color='red', s=10)\n",
        "    if show_center:\n",
        "        ax.scatter([c[0]+c[2]//2 for c in boxes], [c[1]+c[3]//2 for c in boxes], color='green', s=10)\n",
        "    for (xc, yc, w, h), c in zip(boxes, COLORS * 100):\n",
        "        ax.add_patch(plt.Rectangle((xc, yc), w, h,\n",
        "                                   fill=False, color='white', linewidth=2))\n",
        "    if show_probs:\n",
        "        for p, c in zip(prob, centroids):\n",
        "            cl = p.argmax()\n",
        "            text = f'- {p[cl]:0.2f}'\n",
        "            ax.text(c[0] + 15, c[1], text, fontsize=9,\n",
        "                    bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    # plt.savefig(f'images/{file_name}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### before submission run below cell with display_results=True"
      ],
      "metadata": {
        "id": "Xr8fV-NmuqFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mQxrbOICElM"
      },
      "outputs": [],
      "source": [
        "\n",
        "display_results = False\n",
        "show_probs = True\n",
        "show_center = True\n",
        "det_threshold = 0.7\n",
        "\n",
        "model = _make_detr('resnet50', num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "checkpoint = torch.load('checkpoints/checkpoint.pth', map_location='cpu')\n",
        "print(\"DETR returning\", {n:p for n,p in model.named_parameters()}.keys())\n",
        "\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "if not os.path.exists('images'):\n",
        "    os.mkdir('images')\n",
        "\n",
        "coco = COCO(PATH_TO_COCO + 'annotations/instances_val2017.json')\n",
        "ids = coco.getImgIds()\n",
        "\n",
        "val_drift_rates = []\n",
        "for id in range(20):\n",
        "    im = coco.loadImgs(ids[id])[0]\n",
        "    annos = coco.loadAnns(coco.getAnnIds(ids[id]))\n",
        "    img = getImg(im['coco_url']).convert(\"RGB\")\n",
        "    scores, centroids = detect(img, model, transform, det_threshold=det_threshold)\n",
        "    gt_centroids = estimate_centroids(im, annos)\n",
        "    plot_results(img, scores, centroids, gt_centroids, [anno['bbox'] for anno in annos], im['file_name'])\n",
        "    drift_rates = compute_drift_rate(centroids, gt_centroids, [anno['area'] for anno in annos])\n",
        "    print(f'im: {ids[id]}, drift rates : {drift_rates.cpu().numpy()}')\n",
        "    val_drift_rates.append(drift_rates)\n",
        "\n",
        "overall_drit_rates = torch.concat(val_drift_rates, dim=0)\n",
        "print(f'Average drift rate: {overall_drit_rates.mean()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "---\n",
        "**write 1 up to 3 sentences for each question**\n",
        "1. ***What is your intution behind your implementation of  coco.py:convert_coco_poly_to_centroids()*** [2 points]\n",
        "\n",
        "\n",
        "2. ***Which distance metric have you used for computing the cost matrix between centroids? why?*** [1 points]\n",
        "\n",
        "\n",
        "3. ***Why freezing the entire model and traning just the heads might never actually predict the true centroids?*** [2 points]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rIKXjDgMgQPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission guidelines\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "The submission structure should look like:\n",
        "\n",
        "IMPORTANT: Remove the _checkpoint.pth_ and make sure _heads.pth_ is in the submission.\n",
        "\n",
        "\n",
        "```\n",
        "{last name}_{first name}_{sbu id}_hw2/\n",
        "├── part1/\n",
        "│   └── ...\n",
        "├── part2/\n",
        "│   ├── CSE527_23F_HW3_P2.ipynb\n",
        "│   └── detr/\n",
        "│       ├── checkpoints/\n",
        "│       │   └── heads.pth\n",
        "│       ├── datasets/\n",
        "│       │   ├── coco.py\n",
        "│       │   ├── ...\n",
        "│       │   └── ...\n",
        "│       ├── models/\n",
        "│       │   ├── detr.py\n",
        "│       │   ├── matcher.py\n",
        "│       │   ├── ...\n",
        "│       │   └── ...\n",
        "│       ├── utils/\n",
        "│       │   ├── ...\n",
        "│       │   └── ...\n",
        "│       ├── engine.py\n",
        "│       ├── main.py\n",
        "│       ├── hubconf.py\n",
        "│       └── requirements.txt\n",
        "```\n",
        "\n",
        "\n",
        "Follow instructions in part1's submission guidelines to generate your complete submission"
      ],
      "metadata": {
        "id": "Avqzy-BujF9K"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}